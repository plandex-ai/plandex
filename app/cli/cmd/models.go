package cmd

import (
	"fmt"
	"os"
	"plandex-cli/api"
	"plandex-cli/auth"
	"plandex-cli/lib"
	"plandex-cli/term"
	"strconv"
	"strings"

	shared "plandex-shared"

	"github.com/fatih/color"
	"github.com/olekukonko/tablewriter"
	"github.com/spf13/cobra"
)

var customModelsOnly bool

func init() {
	RootCmd.AddCommand(modelsCmd)
	modelsCmd.AddCommand(listAvailableModelsCmd)
	modelsCmd.AddCommand(createCustomModelCmd)
	modelsCmd.AddCommand(deleteCustomModelCmd)
	modelsCmd.AddCommand(defaultModelsCmd)

	listAvailableModelsCmd.Flags().BoolVarP(&customModelsOnly, "custom", "c", false, "List custom models only")
}

var modelsCmd = &cobra.Command{
	Use:   "models",
	Short: "Show plan model settings",
	Run:   models,
}

var defaultModelsCmd = &cobra.Command{
	Use:   "default",
	Short: "Show default model settings for new plans",
	Run:   defaultModels,
}

var listAvailableModelsCmd = &cobra.Command{
	Use:     "available",
	Aliases: []string{"avail"},
	Short:   "List all available models",
	Run:     listAvailableModels,
}

var createCustomModelCmd = &cobra.Command{
	Use:     "add",
	Aliases: []string{"create"},
	Short:   "Add a custom model",
	Run:     createCustomModel,
}

var deleteCustomModelCmd = &cobra.Command{
	Use:     "rm",
	Aliases: []string{"remove", "delete"},
	Short:   "Remove a custom model",
	Args:    cobra.MaximumNArgs(1),
	Run:     deleteCustomModel,
}

func createCustomModel(cmd *cobra.Command, args []string) {
	auth.MustResolveAuthWithOrg()

	model := &shared.AvailableModel{
		BaseModelConfig: shared.BaseModelConfig{
			ModelCompatibility: shared.ModelCompatibility{},
		},
	}

	opts := shared.AllModelProviders
	if auth.Current.IsCloud {
		// remove custom provider if we're in cloud
		filtered := []string{}
		for _, provider := range opts {
			if provider != string(shared.ModelProviderCustom) {
				filtered = append(filtered, provider)
			}
		}
		opts = filtered
	}
	provider, err := term.SelectFromList("Select provider:", opts)

	if err != nil {
		term.OutputErrorAndExit("Error selecting provider: %v", err)
		return
	}
	model.Provider = shared.ModelProvider(provider)

	if model.Provider == shared.ModelProviderCustom {
		if auth.Current.IsCloud {
			term.OutputErrorAndExit("Custom model providers are not supported on Plandex Cloud")
			return
		}
		customProvider, err := term.GetRequiredUserStringInput("Custom provider:")
		if err != nil {
			term.OutputErrorAndExit("Error reading custom provider: %v", err)
			return
		}
		model.CustomProvider = &customProvider
	}

	fmt.Println("For model name, be sure to enter the exact, case-sensitive name of the model as it appears in the provider's API docs. Ex: 'gpt-4o', 'deepseek/deepseek-chat'")
	modelName, err := term.GetRequiredUserStringInput("Model name:")
	if err != nil {
		term.OutputErrorAndExit("Error reading model name: %v", err)
		return
	}
	model.ModelName = shared.ModelName(modelName)

	fmt.Println("For model id, set a unique identifier for the model if you have multiple models with the same name and provider but different settings (otherwise, just press enter to use the model name)")
	modelId, err := term.GetRequiredUserStringInputWithDefault("Model id:", modelName)
	if err != nil {
		term.OutputErrorAndExit("Error reading model id: %v", err)
		return
	}
	model.ModelId = shared.ModelId(modelId)

	fmt.Println("Add a human friendly description if you want to.")
	description, err := term.GetUserStringInput("Description (optional):")
	if err != nil {
		term.OutputErrorAndExit("Error reading description: %v", err)
		return
	}
	model.Description = description

	if model.Provider == shared.ModelProviderCustom {
		baseUrl, err := term.GetRequiredUserStringInput("Base URL:")
		if err != nil {
			term.OutputErrorAndExit("Error reading base URL: %v", err)
			return
		}

		baseUrl = strings.TrimSuffix(baseUrl, "/")

		model.BaseUrl = baseUrl
	} else {
		model.BaseUrl = shared.BaseUrlByProvider[model.Provider]
	}

	apiKeyDefault := shared.ApiKeyByProvider[model.Provider]
	var apiKeyEnvVar string
	if model.Provider == shared.ModelProviderCustom {
		if apiKeyDefault == "" {
			apiKeyEnvVar, err = term.GetRequiredUserStringInput("API key environment variable:")
		} else {
			apiKeyEnvVar, err = term.GetUserStringInputWithDefault("API key environment variable:", apiKeyDefault)
		}
	} else {
		apiKeyEnvVar = apiKeyDefault
	}

	if err != nil {
		term.OutputErrorAndExit("Error reading API key environment variable: %v", err)
		return
	}
	model.ApiKeyEnvVar = apiKeyEnvVar

	fmt.Println("Max Tokens is the total maximum context size of the model.")

	maxTokensStr, err := term.GetRequiredUserStringInput("Max Tokens:")
	if err != nil {
		term.OutputErrorAndExit("Error reading max tokens: %v", err)
		return
	}
	maxTokens, err := strconv.Atoi(maxTokensStr)
	if err != nil {
		term.OutputErrorAndExit("Invalid number for max tokens: %v", err)
		return
	}
	model.MaxTokens = maxTokens

	fmt.Println("'Default Max Convo Tokens' is the default maximum size a conversation can reach in the 'planner' role before it is shortened by summarization. For 128k context, ~10000 is recommended. For 200k context, ~15000 is recommended.")

	var defaultMaxConvoTokens int

	if maxTokens >= 180000 {
		defaultMaxConvoTokens = 15000
	} else if maxTokens >= 100000 {
		defaultMaxConvoTokens = 10000
	}

	var maxConvoTokensStr string
	if defaultMaxConvoTokens == 0 {
		maxConvoTokensStr, err = term.GetRequiredUserStringInput("Default Max Convo Tokens:")
	} else {
		maxConvoTokensStr, err = term.GetRequiredUserStringInputWithDefault("Default Max Convo Tokens:", strconv.Itoa(defaultMaxConvoTokens))
	}

	if err != nil {
		term.OutputErrorAndExit("Error reading max convo tokens: %v", err)
		return
	}
	maxConvoTokens, err := strconv.Atoi(maxConvoTokensStr)
	if err != nil {
		term.OutputErrorAndExit("Invalid number for max convo tokens: %v", err)
		return
	}
	model.DefaultMaxConvoTokens = maxConvoTokens

	fmt.Println("'Max Output Tokens' is the hard limit on output length for the model. Check with the model provider for the recommended value. 8k is a reasonable default if it's not documented, though some models have no max output limitâ€”in that case 'Max Output Tokens' should be set to the same value as 'Max Tokens'.")

	maxOutputTokensStr, err := term.GetRequiredUserStringInputWithDefault("Max Output Tokens:", "8192")
	if err != nil {
		term.OutputErrorAndExit("Error reading reserved output tokens: %v", err)
		return
	}
	maxOutputTokens, err := strconv.Atoi(maxOutputTokensStr)
	if err != nil {
		term.OutputErrorAndExit("Invalid number for reserved output tokens: %v", err)
		return
	}
	model.MaxOutputTokens = maxOutputTokens

	fmt.Println("'Reserved Output Tokens' is the default number of tokens reserved for model output. This ensures the model has enough tokens to generate a response. It can be lower than the 'Max Output Tokens' limit and should be set to what a *realistic* output could reach under normal circumstances. If the 'Max Output Tokens' limit is fairly low, just use that. If the 'Max Output Tokens' is very high, or is equal to the 'Max Tokens' input limit, set a lower value so that there's enough room for input. For reasoning models, make sure enough space is included for reasoning tokens.")

	var defaultReservedOutputTokens int
	if maxOutputTokens <= int(float64(maxTokens)*0.2) {
		defaultReservedOutputTokens = maxOutputTokens
	}

	var reservedOutputTokensStr string
	if defaultReservedOutputTokens == 0 {
		reservedOutputTokensStr, err = term.GetRequiredUserStringInputWithDefault("Reserved Output Tokens:", "8192")
	} else {
		reservedOutputTokensStr, err = term.GetRequiredUserStringInputWithDefault("Reserved Output Tokens:", strconv.Itoa(defaultReservedOutputTokens))
	}

	if err != nil {
		term.OutputErrorAndExit("Error reading reserved output tokens: %v", err)
		return
	}

	reservedOutputTokens, err := strconv.Atoi(reservedOutputTokensStr)
	if err != nil {
		term.OutputErrorAndExit("Invalid number for reserved output tokens: %v", err)
		return
	}
	model.ReservedOutputTokens = reservedOutputTokens

	fmt.Println("'Preferred Output Format' is the format for roles needing structured output. Currently, OpenAI models do best with 'Tool Call JSON' and other models generally do better with 'XML'. Choose 'XML' if you're unsure as it offers the widest compatibility. 'Tool Call JSON' requires tool call support and reliable JSON generation.")

	outputFormatLabels := map[string]string{
		string(shared.ModelOutputFormatXml):          "XML",
		string(shared.ModelOutputFormatToolCallJson): "Tool Call JSON",
	}

	res, err := term.SelectFromList("Preferred Output Format:", []string{
		outputFormatLabels[string(shared.ModelOutputFormatXml)],
		outputFormatLabels[string(shared.ModelOutputFormatToolCallJson)],
	})
	if err != nil {
		term.OutputErrorAndExit("Error selecting output format: %v", err)
		return
	}
	for key, label := range outputFormatLabels {
		if label == res {
			model.PreferredModelOutputFormat = shared.ModelOutputFormat(key)
			break
		}
	}

	model.HasImageSupport, err = term.ConfirmYesNo("Is multi-modal image support enabled?")
	if err != nil {
		term.OutputErrorAndExit("Error confirming image support: %v", err)
		return
	}

	term.StartSpinner("")
	apiErr := api.Client.CreateCustomModel(model)
	term.StopSpinner()

	if apiErr != nil {
		term.OutputErrorAndExit("Error adding model: %v", apiErr.Msg)
		return
	}

	fmt.Println("âœ… Added custom model", color.New(color.Bold, term.ColorHiCyan).Sprint(string(model.Provider)+" â†’ "+string(model.ModelId)))
}

func models(cmd *cobra.Command, args []string) {
	auth.MustResolveAuthWithOrg()
	lib.MustResolveProject()

	if lib.CurrentPlanId == "" {
		term.OutputNoCurrentPlanErrorAndExit()
	}

	term.StartSpinner("")

	plan, err := api.Client.GetPlan(lib.CurrentPlanId)

	if err != nil {
		term.StopSpinner()
		term.OutputErrorAndExit("Error getting plan: %v", err)
		return
	}

	settings, err := api.Client.GetSettings(lib.CurrentPlanId, lib.CurrentBranch)
	term.StopSpinner()

	if err != nil {
		term.OutputErrorAndExit("Error getting settings: %v", err)
		return
	}

	title := fmt.Sprintf("%s Model Settings", color.New(color.Bold, term.ColorHiGreen).Sprint(plan.Name))

	table := tablewriter.NewWriter(os.Stdout)
	table.SetAutoWrapText(false)
	table.Append([]string{title})
	table.Render()
	fmt.Println()

	renderSettings(settings)

	term.PrintCmds("", "set-model", "models available", "models default")
}

func defaultModels(cmd *cobra.Command, args []string) {
	auth.MustResolveAuthWithOrg()

	term.StartSpinner("")
	settings, err := api.Client.GetOrgDefaultSettings()
	term.StopSpinner()

	if err != nil {
		term.OutputErrorAndExit("Error getting default model settings: %v", err)
		return
	}

	title := fmt.Sprintf("%s Model Settings", color.New(color.Bold, term.ColorHiGreen).Sprint("Org-Wide Default"))
	table := tablewriter.NewWriter(os.Stdout)
	table.SetAutoWrapText(false)
	table.Append([]string{title})
	table.Render()
	fmt.Println()

	renderSettings(settings)

	term.PrintCmds("", "set-model default", "models available", "models")
}

func listAvailableModels(cmd *cobra.Command, args []string) {
	auth.MustResolveAuthWithOrg()

	term.StartSpinner("")

	customModels, err := api.Client.ListCustomModels()

	term.StopSpinner()

	if err != nil {
		term.OutputErrorAndExit("Error fetching custom models: %v", err)
		return
	}

	if !customModelsOnly {
		color.New(color.Bold, term.ColorHiCyan).Println("ðŸ  Built-in Models")
		builtIn := shared.AvailableModels
		table := tablewriter.NewWriter(os.Stdout)
		table.SetAutoWrapText(false)
		table.SetHeader([]string{"Provider", "Name", "ðŸª™", "ðŸ”‘"})
		for _, model := range builtIn {
			table.Append([]string{string(model.Provider), string(model.ModelId), strconv.Itoa(model.MaxTokens), model.ApiKeyEnvVar})
		}
		table.Render()
		fmt.Println()
	}

	if len(customModels) > 0 {
		color.New(color.Bold, term.ColorHiCyan).Println("ðŸ› ï¸  Custom Models")
		table := tablewriter.NewWriter(os.Stdout)
		table.SetAutoWrapText(false)
		table.SetHeader([]string{"#", "Provider", "Name", "ðŸª™", "ðŸ”‘"})
		for i, model := range customModels {
			var provider string
			p := model.Provider
			if p == shared.ModelProviderCustom {
				provider = *model.CustomProvider
			} else {
				provider = string(p)
			}

			table.Append([]string{fmt.Sprintf("%d", i+1), provider, string(model.ModelId), strconv.Itoa(model.MaxTokens), model.ApiKeyEnvVar})
		}
		table.Render()
	} else if customModelsOnly {
		fmt.Println("ðŸ¤·â€â™‚ï¸ No custom models")
	}
	fmt.Println()

	if customModelsOnly {
		if len(customModels) > 0 {
			term.PrintCmds("", "models", "set-model", "models add", "models delete")
		} else {
			term.PrintCmds("", "models add")
		}
	} else {
		term.PrintCmds("", "models available --custom", "models", "set-model", "models add", "models delete")
	}
}

func deleteCustomModel(cmd *cobra.Command, args []string) {
	auth.MustResolveAuthWithOrg()

	term.StartSpinner("")
	models, apiErr := api.Client.ListCustomModels()
	term.StopSpinner()

	if apiErr != nil {
		term.OutputErrorAndExit("Error fetching custom models: %v", apiErr)
		return
	}

	if len(models) == 0 {
		fmt.Println("ðŸ¤·â€â™‚ï¸ No custom models")
		fmt.Println()
		term.PrintCmds("", "models add")
		return
	}

	var modelToDelete *shared.AvailableModel

	if len(args) == 1 {
		input := args[0]
		// Try to parse input as index
		index, err := strconv.Atoi(input)
		if err == nil && index > 0 && index <= len(models) {
			modelToDelete = models[index-1]
		} else {
			// Search by name
			for _, m := range models {
				if string(m.ModelId) == input {
					modelToDelete = m
					break
				}
			}
		}
	}

	if modelToDelete == nil {
		opts := make([]string, len(models))
		for i, model := range models {
			var provider string
			p := model.Provider
			if p == shared.ModelProviderCustom {
				provider = *model.CustomProvider
			} else {
				provider = string(p)
			}
			opts[i] = provider + " â†’ " + string(model.ModelId)
		}

		selected, err := term.SelectFromList("Select model to delete:", opts)

		if err != nil {
			term.OutputErrorAndExit("Error selecting model: %v", err)
			return
		}

		var selectedIndex int
		for i, opt := range opts {
			if opt == selected {
				selectedIndex = i
				break
			}
		}

		modelToDelete = models[selectedIndex]
	}

	term.StartSpinner("")
	apiErr = api.Client.DeleteAvailableModel(modelToDelete.Id)
	term.StopSpinner()

	if apiErr != nil {
		term.OutputErrorAndExit("Error deleting custom model: %v", apiErr)
		return
	}

	fmt.Printf("âœ… Deleted custom model %s\n", color.New(color.Bold, term.ColorHiCyan).Sprint(string(modelToDelete.Provider)+" â†’ "+string(modelToDelete.ModelId)))

	fmt.Println()
	term.PrintCmds("", "models available", "models add")
}

func renderSettings(settings *shared.PlanSettings) {
	modelPack := settings.ModelPack

	color.New(color.Bold, term.ColorHiCyan).Println("ðŸŽ›ï¸  Current Model Pack")
	renderModelPack(modelPack)

	color.New(color.Bold, term.ColorHiCyan).Println("ðŸ§  Planner Defaults")
	table := tablewriter.NewWriter(os.Stdout)
	table.SetAutoWrapText(false)
	table.SetHeader([]string{"Max Tokens", "Max Convo Tokens"})
	table.Append([]string{
		fmt.Sprintf("%d", modelPack.Planner.GetFinalLargeContextFallback().BaseModelConfig.MaxTokens),
		fmt.Sprintf("%d", modelPack.Planner.MaxConvoTokens),
	})
	table.Render()
	fmt.Println()

	color.New(color.Bold, term.ColorHiCyan).Println("âš™ï¸  Planner Overrides")
	table = tablewriter.NewWriter(os.Stdout)
	table.SetAutoWrapText(false)
	table.SetHeader([]string{"Name", "Value"})
	if settings.ModelOverrides.MaxTokens == nil {
		table.Append([]string{"Max Tokens", "no override"})
	} else {
		table.Append([]string{"Max Tokens", fmt.Sprintf("%d", *settings.ModelOverrides.MaxTokens)})
	}
	if settings.ModelOverrides.MaxConvoTokens == nil {
		table.Append([]string{"Max Convo Tokens", "no override"})
	} else {
		table.Append([]string{"Max Convo Tokens", fmt.Sprintf("%d", *settings.ModelOverrides.MaxConvoTokens)})
	}
	table.Render()
	fmt.Println()
}

func renderModelPack(modelPack *shared.ModelPack) {
	table := tablewriter.NewWriter(os.Stdout)
	table.SetAutoFormatHeaders(false)
	table.SetAutoWrapText(true)
	table.SetColWidth(64)
	table.SetHeader([]string{modelPack.Name})
	table.Append([]string{modelPack.Description})
	table.Render()
	fmt.Println()

	color.New(color.Bold, term.ColorHiCyan).Println("ðŸ¤– Models")
	table = tablewriter.NewWriter(os.Stdout)
	table.SetAutoWrapText(false)
	table.SetHeader([]string{"Role", "Provider", "Model", "Temperature", "Top P", "Max Input"})
	table.SetColumnAlignment([]int{
		tablewriter.ALIGN_LEFT,  // Role
		tablewriter.ALIGN_LEFT,  // Provider
		tablewriter.ALIGN_LEFT,  // Model
		tablewriter.ALIGN_RIGHT, // Temperature
		tablewriter.ALIGN_RIGHT, // Top P
		tablewriter.ALIGN_RIGHT, // Max Input
	})

	anyRoleParamsDisabled := false

	addModelRow := func(role string, config shared.ModelRoleConfig) {

		var temp float32
		var topP float32
		var disabled bool

		if config.BaseModelConfig.RoleParamsDisabled {
			temp = 1
			topP = 1
			disabled = true
			anyRoleParamsDisabled = true
		} else {
			temp = config.Temperature
			topP = config.TopP
		}

		tempStr := fmt.Sprintf("%.1f", temp)
		if disabled {
			tempStr = "*" + tempStr
		}

		topPStr := fmt.Sprintf("%.1f", topP)
		if disabled {
			topPStr = "*" + topPStr
		}

		table.Append([]string{
			role,
			string(config.BaseModelConfig.Provider),
			string(config.BaseModelConfig.ModelId),
			tempStr,
			topPStr,
			fmt.Sprintf("%d ðŸª™", config.BaseModelConfig.MaxTokens-config.GetReservedOutputTokens()),
		})

		// Add large context and large output fallback(s) if present

		if config.LargeContextFallback != nil {
			if config.LargeContextFallback.BaseModelConfig.RoleParamsDisabled {
				temp = 1
				topP = 1
				disabled = true
				anyRoleParamsDisabled = true
			} else {
				temp = config.LargeContextFallback.Temperature
				topP = config.LargeContextFallback.TopP
			}

			tempStr := fmt.Sprintf("%.1f", temp)
			if disabled {
				tempStr = "*" + tempStr
			}

			topPStr := fmt.Sprintf("%.1f", topP)
			if disabled {
				topPStr = "*" + topPStr
			}

			table.Append([]string{
				"â””â”€ large-context",
				string(config.LargeContextFallback.BaseModelConfig.Provider),
				string(config.LargeContextFallback.BaseModelConfig.ModelId),
				tempStr,
				topPStr,
				fmt.Sprintf("%d ðŸª™", config.LargeContextFallback.BaseModelConfig.MaxTokens-config.LargeContextFallback.GetReservedOutputTokens()),
			})
		}

		if config.LargeOutputFallback != nil {
			if config.LargeOutputFallback.BaseModelConfig.RoleParamsDisabled {
				temp = 1
				topP = 1
				disabled = true
				anyRoleParamsDisabled = true
			} else {
				temp = config.LargeOutputFallback.Temperature
				topP = config.LargeOutputFallback.TopP
			}

			tempStr := fmt.Sprintf("%.1f", temp)
			if disabled {
				tempStr = "*" + tempStr
			}

			topPStr := fmt.Sprintf("%.1f", topP)
			if disabled {
				topPStr = "*" + topPStr
			}

			table.Append([]string{
				"â””â”€ large-output",
				string(config.LargeOutputFallback.BaseModelConfig.Provider),
				string(config.LargeOutputFallback.BaseModelConfig.ModelId),
				tempStr,
				topPStr,
				fmt.Sprintf("%d ðŸª™", config.LargeOutputFallback.BaseModelConfig.MaxTokens-config.LargeOutputFallback.GetReservedOutputTokens()),
			})
		}
	}

	var temp float32
	var topP float32
	var disabled bool

	if modelPack.Planner.BaseModelConfig.RoleParamsDisabled {
		temp = 1
		topP = 1
		disabled = true
		anyRoleParamsDisabled = true
	} else {
		temp = modelPack.Planner.Temperature
		topP = modelPack.Planner.TopP
	}

	tempStr := fmt.Sprintf("%.1f", temp)
	if disabled {
		tempStr = "*" + tempStr
	}

	topPStr := fmt.Sprintf("%.1f", topP)
	if disabled {
		topPStr = "*" + topPStr
	}

	// Handle planner separately since it has a different fallback structure
	table.Append([]string{
		string(shared.ModelRolePlanner),
		string(modelPack.Planner.BaseModelConfig.Provider),
		string(modelPack.Planner.BaseModelConfig.ModelId),
		tempStr,
		topPStr,
		fmt.Sprintf("%d ðŸª™", modelPack.Planner.BaseModelConfig.MaxTokens-modelPack.Planner.GetReservedOutputTokens()),
	})
	if modelPack.Planner.LargeContextFallback != nil {
		var temp float32
		var topP float32
		var disabled bool

		if modelPack.Planner.LargeContextFallback.BaseModelConfig.RoleParamsDisabled {
			temp = 1
			topP = 1
			disabled = true
			anyRoleParamsDisabled = true
		} else {
			temp = modelPack.Planner.LargeContextFallback.Temperature
			topP = modelPack.Planner.LargeContextFallback.TopP
		}

		tempStr := fmt.Sprintf("%.1f", temp)
		if disabled {
			tempStr = "*" + tempStr
		}

		topPStr := fmt.Sprintf("%.1f", topP)
		if disabled {
			topPStr = "*" + topPStr
		}

		table.Append([]string{
			"â””â”€ large-context",
			string(modelPack.Planner.LargeContextFallback.BaseModelConfig.Provider),
			string(modelPack.Planner.LargeContextFallback.BaseModelConfig.ModelId),
			tempStr,
			topPStr,
			fmt.Sprintf("%d ðŸª™", modelPack.Planner.LargeContextFallback.BaseModelConfig.MaxTokens-modelPack.Planner.LargeContextFallback.GetReservedOutputTokens()),
		})
	}

	addModelRow(string(shared.ModelRoleArchitect), modelPack.GetArchitect())
	addModelRow(string(shared.ModelRoleCoder), modelPack.GetCoder())
	addModelRow(string(shared.ModelRolePlanSummary), modelPack.PlanSummary)
	addModelRow(string(shared.ModelRoleBuilder), modelPack.Builder)
	addModelRow(string(shared.ModelRoleWholeFileBuilder), modelPack.GetWholeFileBuilder())
	addModelRow(string(shared.ModelRoleName), modelPack.Namer)
	addModelRow(string(shared.ModelRoleCommitMsg), modelPack.CommitMsg)
	addModelRow(string(shared.ModelRoleExecStatus), modelPack.ExecStatus)
	table.Render()

	if anyRoleParamsDisabled {
		fmt.Println("* these models do not support changing temperature or top p")
	}

	fmt.Println()

}
